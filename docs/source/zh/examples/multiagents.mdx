# ç¼–æ’ multi-agent ç³»ç»Ÿ ğŸ¤–ğŸ¤ğŸ¤–

[[open-in-colab]]

æ­¤notebookå°†æ„å»ºä¸€ä¸ª **multi-agent ç½‘ç»œæµè§ˆå™¨ï¼šä¸€ä¸ªæœ‰å¤šä¸ªä»£ç†åä½œï¼Œä½¿ç”¨ç½‘ç»œè¿›è¡Œæœç´¢è§£å†³é—®é¢˜çš„ä»£ç†ç³»ç»Ÿ**

`ManagedAgent` å¯¹è±¡å°†å°è£…è¿™äº›ç®¡ç†ç½‘ç»œæœç´¢çš„agentï¼Œå½¢æˆä¸€ä¸ªç®€å•çš„å±‚æ¬¡ç»“æ„ï¼š

```
              +----------------+
              | Manager agent  |
              +----------------+
                       |
        _______________|______________
       |                              |
  Code interpreter   +--------------------------------+
       tool          |         Managed agent          |
                     |      +------------------+      |
                     |      | Web Search agent |      |
                     |      +------------------+      |
                     |         |            |         |
                     |  Web Search tool     |         |
                     |             Visit webpage tool |
                     +--------------------------------+
```
æˆ‘ä»¬æ¥ä¸€èµ·æ„å»ºè¿™ä¸ªç³»ç»Ÿã€‚è¿è¡Œä¸‹åˆ—ä»£ç ä»¥å®‰è£…ä¾èµ–åŒ…ï¼š

```
!pip install markdownify duckduckgo-search smolagents --upgrade -q
```

æˆ‘ä»¬éœ€è¦ç™»å½•Hugging Face Hubä»¥è°ƒç”¨HFçš„Inference APIï¼š

```
from huggingface_hub import login

login()
```

âš¡ï¸ HFçš„Inference API å¯ä»¥å¿«é€Ÿè½»æ¾åœ°è¿è¡Œä»»ä½•å¼€æºæ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬çš„agentå°†ä½¿ç”¨HFçš„Inference API
ä¸­çš„`InferenceClientModel`ç±»æ¥è°ƒç”¨
[Qwen/Qwen2.5-Coder-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct)æ¨¡å‹ã€‚

_Note:_ åŸºäºå¤šå‚æ•°å’Œéƒ¨ç½²æ¨¡å‹çš„ Inference API å¯èƒ½åœ¨æ²¡æœ‰é¢„å…ˆé€šçŸ¥çš„æƒ…å†µä¸‹æ›´æ–°æˆ–æ›¿æ¢æ¨¡å‹ã€‚äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[è¿™é‡Œ](https://huggingface.co/docs/api-inference/supported-models)ã€‚

```py
model_id = "Qwen/Qwen2.5-Coder-32B-Instruct"
```

## ğŸ” åˆ›å»ºç½‘ç»œæœç´¢å·¥å…·

è™½ç„¶æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å·²ç»å­˜åœ¨çš„
[`DuckDuckGoSearchTool`](https://github.com/huggingface/smolagents/blob/main/src/smolagents/default_tools.py#L151-L176)
å·¥å…·ä½œä¸ºè°·æ­Œæœç´¢çš„å¹³æ›¿è¿›è¡Œç½‘é¡µæµè§ˆï¼Œç„¶åæˆ‘ä»¬ä¹Ÿéœ€è¦èƒ½å¤ŸæŸ¥çœ‹`DuckDuckGoSearchTool`æ‰¾åˆ°çš„é¡µé¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘
ä»¬å¯ä»¥ç›´æ¥å¯¼å…¥åº“çš„å†…ç½®
`VisitWebpageTool`ã€‚ä½†æ˜¯æˆ‘ä»¬å°†é‡æ–°æ„å»ºå®ƒä»¥äº†è§£å…¶å·¥ä½œåŸç†ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨`markdownify` æ¥ä»å¤´æ„å»ºæˆ‘ä»¬çš„`VisitWebpageTool`å·¥å…·ã€‚

```py
import re
import requests
from markdownify import markdownify
from requests.exceptions import RequestException
from smolagents import tool


@tool
def visit_webpage(url: str) -> str:
    """Visits a webpage at the given URL and returns its content as a markdown string.

    Args:
        url: The URL of the webpage to visit.

    Returns:
        The content of the webpage converted to Markdown, or an error message if the request fails.
    """
    try:
        # Send a GET request to the URL
        response = requests.get(url)
        response.raise_for_status()  # Raise an exception for bad status codes

        # Convert the HTML content to Markdown
        markdown_content = markdownify(response.text).strip()

        # Remove multiple line breaks
        markdown_content = re.sub(r"\n{3,}", "\n\n", markdown_content)

        return markdown_content

    except RequestException as e:
        return f"Error fetching the webpage: {str(e)}"
    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"
```

ç°åœ¨æˆ‘ä»¬åˆå§‹åŒ–è¿™ä¸ªå·¥å…·å¹¶æµ‹è¯•å®ƒï¼

```py
print(visit_webpage("https://en.wikipedia.org/wiki/Hugging_Face")[:500])
```

## æ„å»ºæˆ‘ä»¬çš„ multi-agent ç³»ç»Ÿ ğŸ¤–ğŸ¤ğŸ¤–

ç°åœ¨æˆ‘ä»¬æœ‰äº†æ‰€æœ‰å·¥å…·`search`å’Œ`visit_webpage`ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒä»¬æ¥åˆ›å»ºweb agentã€‚

æˆ‘ä»¬è¯¥é€‰å–ä»€ä¹ˆæ ·çš„é…ç½®æ¥æ„å»ºè¿™ä¸ªagentå‘¢ï¼Ÿ
- ç½‘é¡µæµè§ˆæ˜¯ä¸€ä¸ªå•çº¿ç¨‹ä»»åŠ¡ï¼Œä¸éœ€è¦å¹¶è¡Œå·¥å…·è°ƒç”¨ï¼Œå› æ­¤JSONå·¥å…·è°ƒç”¨å¯¹äºè¿™ä¸ªä»»åŠ¡éå¸¸æœ‰æ•ˆã€‚å› æ­¤æˆ‘ä»¬é€‰æ‹©`ToolCallingAgent`ã€‚
- æœ‰æ—¶å€™ç½‘é¡µæœç´¢éœ€è¦æ¢ç´¢è®¸å¤šé¡µé¢æ‰èƒ½æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆï¼Œæ‰€ä»¥æˆ‘ä»¬æ›´å–œæ¬¢å°† `max_steps` å¢åŠ åˆ°10ã€‚

```py
from smolagents import (
    CodeAgent,
    ToolCallingAgent,
    InferenceClientModel,
    ManagedAgent,
    DuckDuckGoSearchTool,
    LiteLLMModel,
)

model = InferenceClientModel(model_id=model_id)

web_agent = ToolCallingAgent(
    tools=[DuckDuckGoSearchTool(), visit_webpage],
    model=model,
    max_steps=10,
    name="search",
    description="Runs web searches for you. Give it your query as an argument.",
)
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä¸ºè¿™ä¸ªä»£ç†èµ‹äºˆäº† nameï¼ˆåç§°ï¼‰å’Œ descriptionï¼ˆæè¿°ï¼‰å±æ€§ï¼Œè¿™äº›æ˜¯å¿…éœ€å±æ€§ï¼Œä»¥ä¾¿è®©ç®¡ç†ä»£ç†èƒ½å¤Ÿè°ƒç”¨æ­¤ä»£ç†ã€‚

ç„¶åï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®¡ç†ä»£ç†ï¼Œåœ¨åˆå§‹åŒ–æ—¶ï¼Œå°†å—ç®¡ä»£ç†ä½œä¸º managed_agents å‚æ•°ä¼ é€’ç»™å®ƒã€‚

ç”±äºè¿™ä¸ªä»£ç†çš„ä»»åŠ¡æ˜¯è¿›è¡Œè§„åˆ’å’Œæ€è€ƒï¼Œé«˜çº§æ¨ç†èƒ½åŠ›ä¼šå¾ˆæœ‰å¸®åŠ©ï¼Œå› æ­¤ CodeAgentï¼ˆä»£ç ä»£ç†ï¼‰å°†æ˜¯æœ€ä½³é€‰æ‹©ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬è¦æå‡ºä¸€ä¸ªæ¶‰åŠå½“å‰å¹´ä»½å¹¶éœ€è¦è¿›è¡Œé¢å¤–æ•°æ®è®¡ç®—çš„é—®é¢˜ï¼šæ‰€ä»¥è®©æˆ‘ä»¬æ·»åŠ  additional_authorized_imports=["time", "numpy", "pandas"]ï¼Œä»¥é˜²ä»£ç†éœ€è¦ç”¨åˆ°è¿™äº›åŒ…ã€‚

```py
manager_agent = CodeAgent(
    tools=[],
    model=model,
    managed_agents=[web_agent],
    additional_authorized_imports=["time", "numpy", "pandas"],
)
```

å¯ä»¥äº†ï¼ç°åœ¨è®©æˆ‘ä»¬è¿è¡Œæˆ‘ä»¬çš„ç³»ç»Ÿï¼æˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªéœ€è¦ä¸€äº›è®¡ç®—å’Œç ”ç©¶çš„é—®é¢˜ï¼š

```py
answer = manager_agent.run("If LLM training continues to scale up at the current rhythm until 2030, what would be the electric power in GW required to power the biggest training runs by 2030? What would that correspond to, compared to some countries? Please provide a source for any numbers used.")
```

æˆ‘ä»¬ç”¨è¿™ä¸ªreport æ¥å›ç­”è¿™ä¸ªé—®é¢˜ï¼š
```
Based on current growth projections and energy consumption estimates, if LLM trainings continue to scale up at the
current rhythm until 2030:

1. The electric power required to power the biggest training runs by 2030 would be approximately 303.74 GW, which
translates to about 2,660,762 GWh/year.

1. Comparing this to countries' electricity consumption:
   - It would be equivalent to about 34% of China's total electricity consumption.
   - It would exceed the total electricity consumption of India (184%), Russia (267%), and Japan (291%).
   - It would be nearly 9 times the electricity consumption of countries like Italy or Mexico.

2. Source of numbers:
   - The initial estimate of 5 GW for future LLM training comes from AWS CEO Matt Garman.
   - The growth projection used a CAGR of 79.80% from market research by Springs.
   - Country electricity consumption data is from the U.S. Energy Information Administration, primarily for the year
2021.
```

å¦‚æœ[scaling hypothesis](https://gwern.net/scaling-hypothesis)æŒç»­æˆç«‹çš„è¯ï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›åºå¤§çš„åŠ¨åŠ›é…ç½®ã€‚æˆ‘ä»¬çš„agentæˆåŠŸåœ°åä½œè§£å†³äº†è¿™ä¸ªä»»åŠ¡ï¼âœ…

ğŸ’¡ ä½ å¯ä»¥è½»æ¾åœ°å°†è¿™ä¸ªç¼–æ’æ‰©å±•åˆ°æ›´å¤šçš„agentï¼šä¸€ä¸ªæ‰§è¡Œä»£ç ï¼Œä¸€ä¸ªè¿›è¡Œç½‘é¡µæœç´¢ï¼Œä¸€ä¸ªå¤„ç†æ–‡ä»¶åŠ è½½â‹¯â‹¯
