# Agentic RAG

[[open-in-colab]]

Retrieval-Augmented-Generation (RAG) æ˜¯â€œä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å›ç­”ç”¨æˆ·æŸ¥è¯¢ï¼Œä½†åŸºäºä»çŸ¥è¯†åº“ä¸­æ£€ç´¢çš„ä¿¡æ¯â€ã€‚å®ƒæ¯”ä½¿ç”¨æ™®é€šæˆ–å¾®è°ƒçš„ LLM å…·æœ‰è®¸å¤šä¼˜åŠ¿ï¼šä¸¾å‡ ä¸ªä¾‹å­ï¼Œå®ƒå…è®¸å°†ç­”æ¡ˆåŸºäºçœŸå®äº‹å®å¹¶å‡å°‘è™šæ„ï¼›å®ƒå…è®¸æä¾› LLM é¢†åŸŸç‰¹å®šçš„çŸ¥è¯†ï¼›å¹¶å…è®¸å¯¹çŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯è®¿é—®è¿›è¡Œç²¾ç»†æ§åˆ¶ã€‚

ä½†æ˜¯ï¼Œæ™®é€šçš„ RAG å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä»¥ä¸‹ä¸¤ç‚¹å°¤ä¸ºçªå‡ºï¼š

- å®ƒåªæ‰§è¡Œä¸€æ¬¡æ£€ç´¢æ­¥éª¤ï¼šå¦‚æœç»“æœä¸å¥½ï¼Œç”Ÿæˆçš„å†…å®¹ä¹Ÿä¼šä¸å¥½ã€‚
- è¯­ä¹‰ç›¸ä¼¼æ€§æ˜¯ä»¥ç”¨æˆ·æŸ¥è¯¢ä¸ºå‚è€ƒè®¡ç®—çš„ï¼Œè¿™å¯èƒ½ä¸æ˜¯æœ€ä¼˜çš„ï¼šä¾‹å¦‚ï¼Œç”¨æˆ·æŸ¥è¯¢é€šå¸¸æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œè€ŒåŒ…å«çœŸå®ç­”æ¡ˆçš„æ–‡æ¡£é€šå¸¸æ˜¯è‚¯å®šè¯­æ€ï¼Œå› æ­¤å…¶ç›¸ä¼¼æ€§å¾—åˆ†ä¼šæ¯”å…¶ä»–ä»¥ç–‘é—®å½¢å¼å‘ˆç°çš„æºæ–‡æ¡£ä½ï¼Œä»è€Œå¯¼è‡´é”™å¤±ç›¸å…³ä¿¡æ¯çš„é£é™©ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ¶ä½œä¸€ä¸ª RAG  agentæ¥ç¼“è§£è¿™äº›é—®é¢˜ï¼šéå¸¸ç®€å•ï¼Œä¸€ä¸ªé…å¤‡äº†æ£€ç´¢å·¥å…·çš„agentï¼è¿™ä¸ª agent å°†
ä¼šï¼šâœ… è‡ªå·±æ„å»ºæŸ¥è¯¢å’Œæ£€ç´¢ï¼Œâœ… å¦‚æœéœ€è¦çš„è¯ä¼šé‡æ–°æ£€ç´¢ã€‚

å› æ­¤ï¼Œå®ƒå°†æ¯”æ™®é€š RAG æ›´æ™ºèƒ½ï¼Œå› ä¸ºå®ƒå¯ä»¥è‡ªå·±æ„å»ºæŸ¥è¯¢ï¼Œè€Œä¸æ˜¯ç›´æ¥ä½¿ç”¨ç”¨æˆ·æŸ¥è¯¢ä½œä¸ºå‚è€ƒã€‚è¿™æ ·ï¼Œå®ƒå¯ä»¥æ›´
æ¥è¿‘ç›®æ ‡æ–‡æ¡£ï¼Œä»è€Œæé«˜æ£€ç´¢çš„å‡†ç¡®æ€§ï¼Œ [HyDE](https://huggingface.co/papers/2212.10496)ã€‚æ­¤ agent å¯ä»¥
ä½¿ç”¨ç”Ÿæˆçš„ç‰‡æ®µï¼Œå¹¶åœ¨éœ€è¦æ—¶é‡æ–°æ£€ç´¢ï¼Œå°±åƒ [Self-Query](https://docs.llamaindex.ai/en/stable/examples/evaluation/RetryQuery/)ã€‚

æˆ‘ä»¬ç°åœ¨å¼€å§‹æ„å»ºè¿™ä¸ªç³»ç»Ÿ. ğŸ› ï¸

è¿è¡Œä»¥ä¸‹ä»£ç ä»¥å®‰è£…æ‰€éœ€çš„ä¾èµ–åŒ…ï¼š
```bash
!pip install smolagents pandas langchain langchain-community sentence-transformers rank_bm25 --upgrade -q
```

ä½ éœ€è¦ä¸€ä¸ªæœ‰æ•ˆçš„ token ä½œä¸ºç¯å¢ƒå˜é‡ `HF_TOKEN` æ¥è°ƒç”¨ Inference Providersã€‚æˆ‘ä»¬ä½¿ç”¨ python-dotenv æ¥åŠ è½½å®ƒã€‚
```py
from dotenv import load_dotenv
load_dotenv()
```

æˆ‘ä»¬é¦–å…ˆåŠ è½½ä¸€ä¸ªçŸ¥è¯†åº“ä»¥åœ¨å…¶ä¸Šæ‰§è¡Œ RAGï¼šæ­¤æ•°æ®é›†æ˜¯è®¸å¤š Hugging Face åº“çš„æ–‡æ¡£é¡µé¢çš„æ±‡ç¼–ï¼Œå­˜å‚¨ä¸º markdown æ ¼å¼ã€‚æˆ‘ä»¬å°†ä»…ä¿ç•™ `transformers` åº“çš„æ–‡æ¡£ã€‚ç„¶åé€šè¿‡å¤„ç†æ•°æ®é›†å¹¶å°†å…¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ä¸­ï¼Œä¸ºæ£€ç´¢å™¨å‡†å¤‡çŸ¥è¯†åº“ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ [LangChain](https://python.langchain.com/docs/introduction/) æ¥åˆ©ç”¨å…¶å‡ºè‰²çš„å‘é‡æ•°æ®åº“å·¥å…·ã€‚
```py
import datasets
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.retrievers import BM25Retriever

knowledge_base = datasets.load_dataset("m-ric/huggingface_doc", split="train")
knowledge_base = knowledge_base.filter(lambda row: row["source"].startswith("huggingface/transformers"))

source_docs = [
    Document(page_content=doc["text"], metadata={"source": doc["source"].split("/")[1]})
    for doc in knowledge_base
]

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    add_start_index=True,
    strip_whitespace=True,
    separators=["\n\n", "\n", ".", " ", ""],
)
docs_processed = text_splitter.split_documents(source_docs)
```

ç°åœ¨æ–‡æ¡£å·²å‡†å¤‡å¥½ã€‚æˆ‘ä»¬æ¥ä¸€èµ·æ„å»ºæˆ‘ä»¬çš„ agent RAG ç³»ç»Ÿï¼
ğŸ‘‰ æˆ‘ä»¬åªéœ€è¦ä¸€ä¸ª RetrieverToolï¼Œæˆ‘ä»¬çš„ agent å¯ä»¥åˆ©ç”¨å®ƒä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ä¿¡æ¯ã€‚

ç”±äºæˆ‘ä»¬éœ€è¦å°† vectordb æ·»åŠ ä¸ºå·¥å…·çš„å±æ€§ï¼Œæˆ‘ä»¬ä¸èƒ½ç®€å•åœ°ä½¿ç”¨å¸¦æœ‰ `@tool` è£…é¥°å™¨çš„ç®€å•å·¥å…·æ„é€ å‡½æ•°ï¼šå› æ­¤æˆ‘ä»¬å°†éµå¾ª [tools æ•™ç¨‹](../tutorials/tools) ä¸­çªå‡ºæ˜¾ç¤ºçš„é«˜çº§è®¾ç½®ã€‚

```py
from smolagents import Tool

class RetrieverTool(Tool):
    name = "retriever"
    description = "Uses semantic search to retrieve the parts of transformers documentation that could be most relevant to answer your query."
    inputs = {
        "query": {
            "type": "string",
            "description": "The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.",
        }
    }
    output_type = "string"

    def __init__(self, docs, **kwargs):
        super().__init__(**kwargs)
        self.retriever = BM25Retriever.from_documents(
            docs, k=10
        )

    def forward(self, query: str) -> str:
        assert isinstance(query, str), "Your search query must be a string"

        docs = self.retriever.invoke(
            query,
        )
        return "\nRetrieved documents:\n" + "".join(
            [
                f"\n\n===== Document {str(i)} =====\n" + doc.page_content
                for i, doc in enumerate(docs)
            ]
        )

retriever_tool = RetrieverTool(docs_processed)
```
BM25 æ£€ç´¢æ–¹æ³•æ˜¯ä¸€ä¸ªç»å…¸çš„æ£€ç´¢æ–¹æ³•ï¼Œå› ä¸ºå®ƒçš„è®¾ç½®é€Ÿåº¦éå¸¸å¿«ã€‚ä¸ºäº†æé«˜æ£€ç´¢å‡†ç¡®æ€§ï¼Œä½ å¯ä»¥ä½¿ç”¨è¯­ä¹‰æœç´¢ï¼Œä½¿ç”¨æ–‡æ¡£çš„å‘é‡è¡¨ç¤ºæ›¿æ¢ BM25ï¼šå› æ­¤ä½ å¯ä»¥å‰å¾€ [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) é€‰æ‹©ä¸€ä¸ªå¥½çš„åµŒå…¥æ¨¡å‹ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»åˆ›å»ºäº†ä¸€ä¸ªå¯ä»¥ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ä¿¡æ¯çš„å·¥å…·ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°åˆ›å»ºä¸€ä¸ªåˆ©ç”¨è¿™ä¸ª
`retriever_tool` çš„ agentï¼æ­¤ agent å°†ä½¿ç”¨å¦‚ä¸‹å‚æ•°åˆå§‹åŒ–ï¼š
- `tools`ï¼šä»£ç†å°†èƒ½å¤Ÿè°ƒç”¨çš„å·¥å…·åˆ—è¡¨ã€‚
- `model`ï¼šä¸ºä»£ç†æä¾›åŠ¨åŠ›çš„ LLMã€‚

æˆ‘ä»¬çš„ `model` å¿…é¡»æ˜¯ä¸€ä¸ªå¯è°ƒç”¨å¯¹è±¡ï¼Œå®ƒæ¥å—ä¸€ä¸ªæ¶ˆæ¯çš„ list ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›æ–‡æœ¬ã€‚å®ƒè¿˜éœ€è¦æ¥å—ä¸€ä¸ª stop_sequences å‚æ•°ï¼ŒæŒ‡ç¤ºä½•æ—¶åœæ­¢ç”Ÿæˆã€‚ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨åŒ…ä¸­æä¾›çš„ `HfEngine` ç±»æ¥è·å–è°ƒç”¨ Hugging Face çš„ Inference API çš„ LLM å¼•æ“ã€‚

æ¥ç€ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [meta-llama/Llama-3.3-70B-Instruct](meta-llama/Llama-3.3-70B-Instruct) ä½œä¸º llm å¼•
æ“ï¼Œå› ä¸ºï¼š
- å®ƒæœ‰ä¸€ä¸ªé•¿ 128k ä¸Šä¸‹æ–‡ï¼Œè¿™å¯¹å¤„ç†é•¿æºæ–‡æ¡£å¾ˆæœ‰ç”¨ã€‚
- å®ƒåœ¨ HF çš„ Inference API ä¸Šå§‹ç»ˆå…è´¹æä¾›ï¼

_Note:_ æ­¤ Inference API æ‰˜ç®¡åŸºäºå„ç§æ ‡å‡†çš„æ¨¡å‹ï¼Œéƒ¨ç½²çš„æ¨¡å‹å¯èƒ½ä¼šåœ¨æ²¡æœ‰äº‹å…ˆé€šçŸ¥çš„æƒ…å†µä¸‹è¿›è¡Œæ›´æ–°æˆ–æ›¿æ¢ã€‚äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·ç‚¹å‡»[è¿™é‡Œ](https://huggingface.co/docs/api-inference/supported-models)ã€‚

```py
from smolagents import InferenceClientModel, CodeAgent

agent = CodeAgent(
    tools=[retriever_tool], model=InferenceClientModel(model_id="meta-llama/Llama-3.3-70B-Instruct"), max_steps=4, verbose=True
)
```

å½“æˆ‘ä»¬åˆå§‹åŒ– CodeAgent æ—¶ï¼Œå®ƒå·²ç»è‡ªåŠ¨è·å¾—äº†ä¸€ä¸ªé»˜è®¤çš„ç³»ç»Ÿæç¤ºï¼Œå‘Šè¯‰ LLM å¼•æ“æŒ‰æ­¥éª¤å¤„ç†å¹¶ç”Ÿæˆå·¥å…·è°ƒç”¨ä½œä¸ºä»£ç ç‰‡æ®µï¼Œä½†ä½ å¯ä»¥æ ¹æ®éœ€è¦æ›¿æ¢æ­¤æç¤ºæ¨¡æ¿ã€‚æ¥ç€ï¼Œå½“å…¶ `.run()` æ–¹æ³•è¢«è°ƒç”¨æ—¶ï¼Œä»£ç†å°†è´Ÿè´£è°ƒç”¨ LLM å¼•æ“ï¼Œå¹¶åœ¨å¾ªç¯ä¸­æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼Œç›´åˆ°å·¥å…· `final_answer` è¢«è°ƒç”¨ï¼Œè€Œå…¶å‚æ•°ä¸ºæœ€ç»ˆç­”æ¡ˆã€‚

```py
agent_output = agent.run("For a transformers model training, which is slower, the forward or the backward pass?")

print("Final output:")
print(agent_output)
```
